## 9장 웹 로봇

## 2. 로봇의 HTTP

- 로봇들은 다른 HTTP 클라이언트 프로그램과 다르지 않기 때문에 HTTP 명세를 지켜야 함
  - HTTP 요청을 만들며, 스스로를 HTTP/1.1 클라이언트라고 내세우기 때문에 적절한 HTTP 요청 헤더를 사용해야 함
- 많은 로봇들이 HTTP를 최소한으로만 구현하고자 함
  - 이 때문에 HTTP/1.0 요청을 보내는 로봇들이 많음

### 2.1 요청 헤더 식별하기

- HTTP를 최소한으로 지원하더라도, 신원 식별 헤더(특히 User-Agent HTTP 헤더)를 구현하고 전송해야 함
  - 로봇의 능력, 신원, 출신을 알려주는 기본적인 몇 가지 헤더들을 웹 사이트에게 보내줘야 함
- 이는 서버 측에서 잘못된 크롤러 사용자 검증과, 로봇이 다룰 수 있는 컨텐츠에 대한 정보를 얻기에 유용하게 해줌

**User-Agent**

- 서버에게 요청을 만든 로봇의 이름을 말해줌

**From**

- 로봇 사용자/관리자의 이메일 주소 제공(RFC 822 이메일 주소 포맷)

**Accept**

- 서버가 어떤 미디어 타입을 줄 수 있는지 말해줌

**Referer**

- 현재 요청 URL을 포함한 문서 URL 제공
- 서버로 하여금 어디에서 웹 사이트 링크를 찾아낼 수 있었는지에 대한 단서를 제공해줌

### 2.2 가상 호스팅

- 로봇 구현자들은 Host 헤더를 지원할 필요가 있음
- 가상 호스팅이 널리 퍼져있기 때문에, Host 헤더가 없으면 로봇은 URL에 대한 잘못된 컨텐츠를 찾게 됨
  - 이 때문에 HTTP/1.1은 Host 헤더 사용을 요구함

<img width="748" alt="Host 헤더의 부재" src="https://user-images.githubusercontent.com/75058239/202880621-73f0612f-340e-4a60-94cb-f78e81d727e2.png">

### 2.3 조건부 요청

- 수십억 개의 웹 페이지를 받아야 하는 인터넷 검색엔진 로봇의 경우, 오직 변경되었을 때만 컨텐츠를 가져오도록 할 수 있음
- 로봇은 받아둔 마지막 버전 이후 업데이트 사항이 있는지 알아보는 조건부 HTTP 요청을 구현함
  - HTTP 캐시가 리소스의 로컬 사본에 대한 유효성을 검사하는 방법과 매우 비슷

### 2.4 응답 다루기

- 사실 대다수 로봇들은 GET 메소드 위주로 사용하기 때문에 딱히 응답을 다룰 일이 없긴 함
- 그러나 조건부 요청을 사용하거나, 서버와의 상호작용을 더 잘 해보려고 하는 로봇들은 HTTP 응답을 다룰 줄 알아야 함

**_상태 코드_**

- 로봇들은 최소한 일반적인 상태 코드들은 다룰 수 있어야 함
  - 특히 200 OK, 404 Not Found
- 로봇이 명시적으로 이해할 수 없는 상태 코드는, 해당 상태 코드가 속한 분류에 근거해서 다뤄야 함
- 모든 서버가 항상 적절한 에러 코드를 반환하지 않는다는 점에 유의해야 함
  - 에러를 기술하는 메시지를 200 OK로 응답하는 서버도 있음
  - 이에 대비해서 뭔가 할 수 있는 일은 거의 없지만 명세를 구현하는 개발자라면 알아두고 있어야 함

**_엔티티_**

- HTTP 헤더에 임베딩된 정보를 따라서 로봇들은 엔티티 자체의 정보를 찾을 수 있음
- 메타 http-equiv 태그와 같은 메타 HTML 태그는 리소스에 대한 컨텐츠 저자가 포함시킨 정보
- http-equiv 태그 자체는 컨텐츠를 다루는 서버가 제공할 수도 있는 헤더를 덮어쓰기 위한 수단

```html
<meta http-equiv="Refresh" content="1; URL=index.html" />
```

※ Refresh 헤더를 HTML 내에서 제공해준다.

- http-equiv 태그를 헤더로 포함시키는 서버가 있는 반면, 그렇지 않은 서버도 있음
- 로봇 구현자는 HTML 문서의 HEAD 태그에서 http-equiv 정보를 찾고자 할 수 있음

### 2.5 User-Agent 타게팅

- 웹 관리자들은 로봇들로부터의 요청을 예상해야 함
- 서버에서 브라우저의 종류에 맞게 컨텐츠를 최적화하는 경우,<br>로봇에게 "your browser does not support frames"라는 에러 문구를 전송하게 될 수 있음
- 그러므로 사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야 함
  - 몇몇 브라우저에 특화된 컨텐츠를 개발하는 대신, 보다 다양한 클라이언트에게 대응할 수 있는 유연한 페이지 개발
  - 최소한 로봇이 방문했는데 컨텐츠를 못 얻게 되는 일이 없도록 대비해야 함

<br>

## 3. 부적절하게 동작하는 로봇들

**폭주하는 로봇**

- 로봇은 웹 서핑을 하는 사람보다 훨씬 빠르게 HTTP 요청을 만들 수 있음
  - 게다가 빠른 네트워크 연결을 갖춘 빠른 컴퓨터에서 동작하고 있을 가능성이 높음
- 빠른 속도를 가졌기 때문에 논리적 에러나 순환에 빠졌을 때는 웹 서버에 극심한 과부하를 안겨주게 될 수도 있음
- 그렇기 때문에 로봇 구현자들은 폭주 방지를 위한 보호 장치를 마련해야만 함

**오래된 URL**

- 로봇은 URL의 목록을 방문하는데, 해당 목록이 오래되었을 수 있음
- 웹 사이트의 컨텐츠가 많이 바뀌었다면, 로봇은 존재하지 않는 URL에 대한 요청을 많이 보내게 될 수도 있음
- 이 또한 에러 페이지 제공으로 인한 서버 부하로 이어질 수 있기 때문에 웹 사이트 관리자의 짜증을 유발함

**길고 잘못된 URL**

- 순환이나 로직상의 오류로 로봇은 웹 사이트에게 크고 의미 없는 URL을 요청하게 될 수도 있음
- 이는 웹 서버 처리 능력에 영향을 주고, 웹 서버의 접근 로그를 어지럽게 채워버리고, 허술한 웹 서버라면 고장을 낼 수도 있음

**호기심이 지나친 로봇**

- 어떤 로봇들은 사적인 데이터에 대한 URL을 얻어서 인터넷 검색엔진에서도 접근할 수 있도록 만들 수도 있음
  - 최악의 경우 사생활 침해라고 여겨질 수 있음
- 보통은 사적 컨텐츠에 대한 하이퍼링크를 잘 감춰두지 않아서 접근이 되는 것이긴 한데,<br>매우 광적인 로봇은 하이퍼링크가 명시적으로 존재하지도 않는 문서들을 디렉토리의 컨텐츠를 가져오는 등의 방법으로 찾아내기도 함
- 로봇 구현자는 비밀번호 파일, 신용카드 정보 등 민감한 데이터를 로봇이 검색하지 않도록 주의해야 함
- 사실, 인터넷에 링크가 존재하는 한 진정한 의미의 사적인 리소스는 거의 없음

**동적 게이트웨이 접근**

- 로봇들은 그들이 접근하고 있는 것에 대해 언제나 잘 알고 있는 것은 아님
- 로봇은 게이트웨이 애플리케이션의 컨텐츠에 대한 URL로 요청하게 될 수도 있음
  - 이 경우 데이터는 아마 특수 목적을 위한 것일 테고 처리 비용이 많이 듦
- 많은 웹 사이트 관리자들은 게이트웨이에서 얻은 문서를 요청하는 순진한 로봇들을 좋아하지 않음
