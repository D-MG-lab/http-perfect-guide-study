## 9장 웹 로봇

- 웹 로봇은 자동으로 웹 트랜잭션들을 연속적으로 수행하는 소프트웨어 프로그램이다.
- 방식에 따라 '크롤러', ' 스파이더', '웜', '봇' 등의 다양한 이름으로 불린다.

## 1. 크롤러와 크롤링

- 웹 크롤러는 반복적으로 웹 페이지를 가져오는 웹 순회 로봇이다.
- 가져온 문서들은 검색 가능한 데이터베이스로 바뀌게 되고, 이러한 페이지들이 수십억 개가 되어 크롤러는 가장 복잡한 로봇 중 하나가 되었다.

### 1.1 어디에 시작하는가: '루트 집합'

- 크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합이라 한다.
- 웹 페이지들을 커버하기 위하여 루트 집합에 너무 많은 페이지가 있을 필요는 없고, 다른 페이지로 연결이 가능한(그러나 각 루트 집합에 모인 페이지는 서로 연결고리가 없는) 페이지가 있으면 된다.
- 일반적으로 좋은 루트 집합에는 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 잘 알려져 있지 않은 페이지들의 목록으로 구성되어 있다.

### 1.2 링크 추출과 상대 링크 정상화

- 크롤러는 검색한 각 페이지 내 URL 링크들을 파싱해서 크롤링할 페이지들의 목록애 추가하는데, 새 링크를 발견함에 따라 목록은 급속히 확장된다.

### 1.3 순환 피하기

- A -> B, B -> C, C -> A 처럼 각 링크된 페이지를 크롤러가 가져온다면 A, B, C 페이지들을 반복적으로 가져오는 순환에 빠지게 된다.
- 순환을 피하기 위해 크롤러가 어디에 방문했는지 알아야 한다.

### 1.4 루프와 중복

- 순환은 3가지 이유로 크롤러에게 해롭다.
  - 순환은 크롤러를 루프에 빠뜨려 꼼짝 못하게 만들 수 있는데, 같은 페이지들을 반복해서 가져오게되면서 네트워크 대역폭을 다 차지하고 어떤 페이지도 가져올 수 없게 될 수도 있다.
  - 같은 페이지를 반복해서 가져오면 웹 서버 부담이 커지며, 웹 사이트를 압박하여 실제 사용자들이 사이트 접근을 할 수 없게 될 수도 있다.
  - 많은 수의 중복된 페이지들을 가져오면 결국 똑같은 페이지를 반환하는 인터넷 검색엔진이 된다.

### 1.5 빵 부스러기의 흔적

- 크롤링한 URL들은 굉장히 많기 때문에 어떤 URL을 방문했는지 판단하는 자료 구조를 사용할 필요가 있고, 속도와 메모리 사용 면에서 효과적이어야 한다.
- 로봇이 어떤 URL을 방문했는지 빠르게 결정하기 위해 검색 트리나 해시 테이블을 필요로 한다.
- 수억 개의 URL은 많은 공간을 차지한다.

**트리와 해시 테이블**

- 복잡한 로봇이라면 방문한 URL을 추적하기 위해 검색 트리나 해시 테이블을 사용했을 수도 있다.

**느슨한 존재 비트맵**

- 공간 사용의 최소화를 위해 존재 비트 배열(presence bit array)와 같은 느슨한 자료 구조를 사용한다.
  - 존재 비트는 각 URL을 해시 함수에 의해 고정된 크기의 숫자로 변환하여 배열 안에 대응하도록 하는 것

**체크포인트**

- 로봇 프로그램이 갑자기 중단될 경우를 대비하여 방문한 URL목록이 디스크에 저장되었는지 확인한다.

**파티셔닝**

- 한 대의 컴퓨터에서 하나의 로봇으로 크롤링을 완수하기에는 메모리, 디스크 공간, 연산 능력, 네트워크 대역폭이 충분하지 못할 수 있기 때문에, 각 분리된 여러 로봇들이 동시에 일하게 만든다.(farm)

### 1.6 별칭(alias)과 로봇 순환

- URL이 별칭을 가질 수 있기 때문에 올바른 자료 구조를 갖추었더라도 어떤 페이지를 이전에 방문했었는지 확인이 쉽지 않을 때도 있다.
  ![같은 문서를 가리키는 다른 URL들](https://user-images.githubusercontent.com/74203440/203156783-9f9c4086-fea8-4c14-9a9f-9edb340d39be.png)

### 1.7 URL 정규화하기

- 대부분의 웹 로봇들은 URL들을 표준 형식으로 '정규화' 함으로써 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거한다.

  - 1. 포트번호가 명시되지 않았다면, 호스트 명에 ':80'을 추가한다.
  - 2. 모든 %xx 이스케이핑된 문자들을 대응하는 문자로 변환한다.
  - 3. #태그들을 제거한다.

- URL 정규화로 기본적인 문법의 별칭을 제거할 수 있지만, 제거할 수 없는 다른 URL 별칭도 있다.

### 1.8 파일 시스템 링크 순환

- 심벌릭 링크는 디렉터리 계층을 만들어 끝없는 순환을 유발할 수 있다.
  ![심벌릭 링크 사이클](https://user-images.githubusercontent.com/74203440/203156818-e22cfa4f-e553-42c5-84da-18cc8296a963.png)

### 1.9 동적 가상 웹 공간

- 게이트웨이 애플리케이션은 같은 서버에 있는 가상의 URL에 대한 링크를 포함한 HTML을 즉석에서 만들어 낼 수 있다. 이러한 링크는 실제로 파일을 하나도 갖고 있지 않을 수도 있다. URL과 HTML이 매번 달라 보일 수 있기 때문에 로봇이 순환을 감지하기가 매우 어렵다.
  ![악의적인 동적 웹 공간의 예](https://user-images.githubusercontent.com/74203440/203156804-5eb174f0-fb32-459e-90dd-3a633755311a.jpeg)

### 1.10 루프와 중복 피하기

- 모든 순환을 피하는 완벽한 방법은 없고 웹에서 로봇이 문제를 일으킬 가능성은 많다.

**URL 정규화**

- URL을 표준 형태로 변환하여 같은 리소스를 가리키는 중복 URL을 거른다.

**너비 우선 크롤링**

- 방문할 URL들을 너비 우선으로 스케쥴링하면 순환의 영향을 최소화 할 수 있다.

**스로틀링**

- 웹 사이트의 별칭들에 지속적인 접근을 시도한다면, 스로틀링을 이용해 일정 시간동안의 해당 서버에 대한 접근 횟수와 중복의 총 횟수를 제한할 수 있다.

**URL 크기 제한**

- 보통 1KB를 넘는 긴 URL의 크롤링을 거부하는 방법이 있다. 길이 제한으로 순환이 중단될 수 있다.
- 이 기법은 가져오지 못하는 콘텐츠가 생길 수 있지만, URL이 특정 크기에 도달할 때마다 에러 로그를 남김으로써 특정 사이트에서 어떤 일이 벌어지는지 감시하는 사용자에게는 훌륭한 신호를 제공할 수 있다.

**URL/사이트 블랙리스트**

- 사람이 직접 개입하여 로봇 순환을 만들거나 함정인 사이트의 URL 목록을 만들어 관리하고 피한다.
- 대규모 크롤러들은 악의적이거나 문제를 내재한 것이 확실한 사이트들을 피하기 위해 사용하는 몇 가지 형태의 블랙리스트를 가지고 있다.

**패턴 발견**

- 심벌릭 링크를 통한 순환과 비슷한 오설정들은 일정 패턴을 따르는 경향이 있기 때문에 반복되는 구성요소를 가진 URL을 잠재적인 순환으로 보고 크롤링 하는 것을 거절한다.

**콘텐츠 지문(fingerprint)**

- 콘텐츠 지문을 사용하는 로봇들은 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬(checksum)을 계산한다.
- 이전에 보았던 체크섬을 가진 페이지를 가져오면 그 페이지는 크롤링하지 않는다.
- 지문 생성용으로 MD5와 같은 메시지 요약 함수가 있다.
- 웹 서버가 동적으로 페이지를 수정하는 경우 체크섬 계산에서 빠뜨리게 되어 중복 감지를 방해할 수 있다.

## 2. 로봇의 HTTP

- 로봇들은 다른 HTTP 클라이언트 프로그램과 다르지 않기 때문에 HTTP 명세를 지켜야 함
  - HTTP 요청을 만들며, 스스로를 HTTP/1.1 클라이언트라고 내세우기 때문에 적절한 HTTP 요청 헤더를 사용해야 함
- 많은 로봇들이 HTTP를 최소한으로만 구현하고자 함
  - 이 때문에 HTTP/1.0 요청을 보내는 로봇들이 많음

### 2.1 요청 헤더 식별하기

- HTTP를 최소한으로 지원하더라도, 신원 식별 헤더(특히 User-Agent HTTP 헤더)를 구현하고 전송해야 함
  - 로봇의 능력, 신원, 출신을 알려주는 기본적인 몇 가지 헤더들을 웹 사이트에게 보내줘야 함
- 이는 서버 측에서 잘못된 크롤러 사용자 검증과, 로봇이 다룰 수 있는 컨텐츠에 대한 정보를 얻기에 유용하게 해줌

**User-Agent**

- 서버에게 요청을 만든 로봇의 이름을 말해줌

**From**

- 로봇 사용자/관리자의 이메일 주소 제공(RFC 822 이메일 주소 포맷)

**Accept**

- 서버가 어떤 미디어 타입을 줄 수 있는지 말해줌

**Referer**

- 현재 요청 URL을 포함한 문서 URL 제공
- 서버로 하여금 어디에서 웹 사이트 링크를 찾아낼 수 있었는지에 대한 단서를 제공해줌

### 2.2 가상 호스팅

- 로봇 구현자들은 Host 헤더를 지원할 필요가 있음
- 가상 호스팅이 널리 퍼져있기 때문에, Host 헤더가 없으면 로봇은 URL에 대한 잘못된 컨텐츠를 찾게 됨
  - 이 때문에 HTTP/1.1은 Host 헤더 사용을 요구함

<img width="748" alt="Host 헤더의 부재" src="https://user-images.githubusercontent.com/75058239/202880621-73f0612f-340e-4a60-94cb-f78e81d727e2.png">

### 2.3 조건부 요청

- 수십억 개의 웹 페이지를 받아야 하는 인터넷 검색엔진 로봇의 경우, 오직 변경되었을 때만 컨텐츠를 가져오도록 할 수 있음
- 로봇은 받아둔 마지막 버전 이후 업데이트 사항이 있는지 알아보는 조건부 HTTP 요청을 구현함
  - HTTP 캐시가 리소스의 로컬 사본에 대한 유효성을 검사하는 방법과 매우 비슷

### 2.4 응답 다루기

- 사실 대다수 로봇들은 GET 메소드 위주로 사용하기 때문에 딱히 응답을 다룰 일이 없긴 함
- 그러나 조건부 요청을 사용하거나, 서버와의 상호작용을 더 잘 해보려고 하는 로봇들은 HTTP 응답을 다룰 줄 알아야 함

**_상태 코드_**

- 로봇들은 최소한 일반적인 상태 코드들은 다룰 수 있어야 함
  - 특히 200 OK, 404 Not Found
- 로봇이 명시적으로 이해할 수 없는 상태 코드는, 해당 상태 코드가 속한 분류에 근거해서 다뤄야 함
- 모든 서버가 항상 적절한 에러 코드를 반환하지 않는다는 점에 유의해야 함
  - 에러를 기술하는 메시지를 200 OK로 응답하는 서버도 있음
  - 이에 대비해서 뭔가 할 수 있는 일은 거의 없지만 명세를 구현하는 개발자라면 알아두고 있어야 함

**_엔티티_**

- HTTP 헤더에 임베딩된 정보를 따라서 로봇들은 엔티티 자체의 정보를 찾을 수 있음
- 메타 http-equiv 태그와 같은 메타 HTML 태그는 리소스에 대한 컨텐츠 저자가 포함시킨 정보
- http-equiv 태그 자체는 컨텐츠를 다루는 서버가 제공할 수도 있는 헤더를 덮어쓰기 위한 수단

```html
<meta http-equiv="Refresh" content="1; URL=index.html" />
```

※ Refresh 헤더를 HTML 내에서 제공해준다.

- http-equiv 태그를 헤더로 포함시키는 서버가 있는 반면, 그렇지 않은 서버도 있음
- 로봇 구현자는 HTML 문서의 HEAD 태그에서 http-equiv 정보를 찾고자 할 수 있음

### 2.5 User-Agent 타게팅

- 웹 관리자들은 로봇들로부터의 요청을 예상해야 함
- 서버에서 브라우저의 종류에 맞게 컨텐츠를 최적화하는 경우,<br>로봇에게 "your browser does not support frames"라는 에러 문구를 전송하게 될 수 있음
- 그러므로 사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야 함
  - 몇몇 브라우저에 특화된 컨텐츠를 개발하는 대신, 보다 다양한 클라이언트에게 대응할 수 있는 유연한 페이지 개발
  - 최소한 로봇이 방문했는데 컨텐츠를 못 얻게 되는 일이 없도록 대비해야 함

<br>

## 3. 부적절하게 동작하는 로봇들

**폭주하는 로봇**

- 로봇은 웹 서핑을 하는 사람보다 훨씬 빠르게 HTTP 요청을 만들 수 있음
  - 게다가 빠른 네트워크 연결을 갖춘 빠른 컴퓨터에서 동작하고 있을 가능성이 높음
- 빠른 속도를 가졌기 때문에 논리적 에러나 순환에 빠졌을 때는 웹 서버에 극심한 과부하를 안겨주게 될 수도 있음
- 그렇기 때문에 로봇 구현자들은 폭주 방지를 위한 보호 장치를 마련해야만 함

**오래된 URL**

- 로봇은 URL의 목록을 방문하는데, 해당 목록이 오래되었을 수 있음
- 웹 사이트의 컨텐츠가 많이 바뀌었다면, 로봇은 존재하지 않는 URL에 대한 요청을 많이 보내게 될 수도 있음
- 이 또한 에러 페이지 제공으로 인한 서버 부하로 이어질 수 있기 때문에 웹 사이트 관리자의 짜증을 유발함

**길고 잘못된 URL**

- 순환이나 로직상의 오류로 로봇은 웹 사이트에게 크고 의미 없는 URL을 요청하게 될 수도 있음
- 이는 웹 서버 처리 능력에 영향을 주고, 웹 서버의 접근 로그를 어지럽게 채워버리고, 허술한 웹 서버라면 고장을 낼 수도 있음

**호기심이 지나친 로봇**

- 어떤 로봇들은 사적인 데이터에 대한 URL을 얻어서 인터넷 검색엔진에서도 접근할 수 있도록 만들 수도 있음
  - 최악의 경우 사생활 침해라고 여겨질 수 있음
- 보통은 사적 컨텐츠에 대한 하이퍼링크를 잘 감춰두지 않아서 접근이 되는 것이긴 한데,<br>매우 광적인 로봇은 하이퍼링크가 명시적으로 존재하지도 않는 문서들을 디렉토리의 컨텐츠를 가져오는 등의 방법으로 찾아내기도 함
- 로봇 구현자는 비밀번호 파일, 신용카드 정보 등 민감한 데이터를 로봇이 검색하지 않도록 주의해야 함
- 사실, 인터넷에 링크가 존재하는 한 진정한 의미의 사적인 리소스는 거의 없음

**동적 게이트웨이 접근**

- 로봇들은 그들이 접근하고 있는 것에 대해 언제나 잘 알고 있는 것은 아님
- 로봇은 게이트웨이 애플리케이션의 컨텐츠에 대한 URL로 요청하게 될 수도 있음
  - 이 경우 데이터는 아마 특수 목적을 위한 것일 테고 처리 비용이 많이 듦
- 많은 웹 사이트 관리자들은 게이트웨이에서 얻은 문서를 요청하는 순진한 로봇들을 좋아하지 않음

(승호님이 해주실 9.4 내용 부분)

## 5. 로봇 에티켓

- 1993년, 웹 로봇 커뮤니티의 개척자 Martijn Koster는 웹 로봇 제작자를 위한 가이드라인 목록을 작성했음
- 조언 중 몇 가지는 구식이 되어버렸지만, 대다수는 아직도 상당히 유용함
- [Guidelines for Robot Writers](https://www.robotstxt.org/guidelines.html)

<br>

## 6. 검색엔진

- 웹 로봇을 가장 광범위하게 사용하는 주체는 인터넷 검색엔진
- 인터넷 검색엔진은 사용자가 전 세계 어떤 주제에 대한 문서라도 찾을 수 있게끔 해줌
- 오늘날 가장 유명한 웹 사이트 중 상당수가 검색엔진
  - 많은 웹 사용자들의 시작점이 되면서 사용자들이 관심 있는 정보를 찾을 수 있도록 도와줌
- 웹 크롤러들은 검색엔진에게 문서들을 가져다줌으로써, 어떤 문서에 어떤 단어들이 존재하는지에 대한 색인을 생성할 수 있게 함

### 6.1 넓게 생각하라

- 수백만 명의 사용자들이 수십억 개의 웹페이지에서 원하는 정보를 찾는 상황에서,<br>복잡한 질의 엔진도 필요하고, 복잡한 크롤러도 사용해야 함
- HTTP 질의 요청이 완료되는 데에 0.5초가 걸린다면 수십억 개의 페이지들을 검색하기 위해선 5,700일이 걸림

### 6.2 현대적인 검색엔진의 아키텍처

- 오늘날 검색엔진들은 전 세계 웹 페이지들에 대해 _full-text indexes_ 라고 하는 복잡한 로컬 DB를 생성함
- 이 색인은 웹의 모든 문서에 대해 일종의 카드 카탈로그처럼 동작함

(이미지 첨부 - full-text indexes)

- 검색엔진 크롤러들은 웹페이지들을 수집해와서 full-text indexes에 추가함
- 검색엔진 사용자들은 [hotbot](http://www.hotbot.com)이나 [google](http://www.google.com) 같은 웹 검색 게이트웨이를 통해 full-text indexes에 대한 질의를 보냄
- 웹 페이지들은 매 순간 변하기 때문에 full-text indexes는 기껏해야 웹의 특정 순간에 대한 스냅샷에 불과함

### 6.3 full-text indexes

- 단어 하나를 입력받아서 해당 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스
- 데이터베이스 안의 문서들은 색인 생성 후에는 검색할 필요가 없음

### 6.4 질의 보내기

사용자가 질의를 웹 검색엔진 게이트웨이로 보내는 방법

- HTML 폼을 사용자가 채워 넣고, 브라우저가 해당 폼을 HTTP GET이나 POST 요청을 통해서 게이트웨이로 보내는 식
- 게이트웨이 프로그램은 검색 쿼리를 추출하고, 해당 질의를 full-text indexes를 검색할 때 사용되는 표현식으로 변환함

### 6.5 검색 결과를 정렬하고 보여주기

- 색인을 이용한 결과가 나왔다면, 게이트웨이 애플리케이션은 최종 사용자를 위한 결과 페이지를 즉석에서 만들어냄
- 검색엔진은 결과들에 순위를 매기기 위해 똑똑한 알고리듬을 사용함
- 주어진 단어와 가장 관련이 많은 순서대로 결과 문서에 나타나게 하며, 이를 _relevancy ranking_ 이라고 부름
- 검색 결과의 목록에 점수를 매기고 정렬하는 과정
- 이 과정을 보조하기 위해, 많은 검색엔진이 크롤링 과정에서 수집된 통계 데이터를 실제로 사용함
- 검색엔진의 알고리듬, 크롤링 팁, 그 외 각종 기교는 검색엔진의 가장 엄격히 감추어진 비밀들

### 6.6 스푸핑

- 사용자들은 원하는 결과가 최상위 몇 줄에 보이지 않으면 불만족하므로, 검색 결과의 순서는 중요함
- 웹 마스터는 검색 결과 상단에 노출되도록 만들 만한 동기가 충분히 주어진 셈
- 반면에 검색엔진을 속이는 가짜 페이지 등을 생성해서 악용하는 케이스도 있음
- 결국 검색엔진과 로봇 구현자들은 속임수들을 더 잘 잡아내기 위해 끊임없이 relevancy ranking 알고리듬을 수정해야 함
