## 9장 웹 로봇

- 웹 로봇은 자동으로 웹 트랜잭션들을 연속적으로 수행하는 소프트웨어 프로그램이다.
- 방식에 따라 '크롤러', ' 스파이더', '웜', '봇' 등의 다양한 이름으로 불린다.

## 1. 크롤러와 크롤링

- 웹 크롤러는 반복적으로 웹 페이지를 가져오는 웹 순회 로봇이다.
- 가져온 문서들은 검색 가능한 데이터베이스로 바뀌게 되고, 이러한 페이지들이 수십억 개가 되어 크롤러는 가장 복잡한 로봇 중 하나가 되었다.

### 1.1 어디에 시작하는가: '루트 집합'

- 크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합이라 한다.
- 웹 페이지들을 커버하기 위하여 루트 집합에 너무 많은 페이지가 있을 필요는 없고, 다른 페이지로 연결이 가능한(그러나 각 루트 집합에 모인 페이지는 서로 연결고리가 없는) 페이지가 있으면 된다.
- 일반적으로 좋은 루트 집합에는 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 잘 알려져 있지 않은 페이지들의 목록으로 구성되어 있다.

### 1.2 링크 추출과 상대 링크 정상화

- 크롤러는 검색한 각 페이지 내 URL 링크들을 파싱해서 크롤링할 페이지들의 목록애 추가하는데, 새 링크를 발견함에 따라 목록은 급속히 확장된다.

### 1.3 순환 피하기

- A -> B, B -> C, C -> A 처럼 각 링크된 페이지를 크롤러가 가져온다면 A, B, C 페이지들을 반복적으로 가져오는 순환에 빠지게 된다.
- 순환을 피하기 위해 크롤러가 어디에 방문했는지 알아야 한다.

### 1.4 루프와 중복

- 순환은 3가지 이유로 크롤러에게 해롭다.
  - 순환은 크롤러를 루프에 빠뜨려 꼼짝 못하게 만들 수 있는데, 같은 페이지들을 반복해서 가져오게되면서 네트워크 대역폭을 다 차지하고 어떤 페이지도 가져올 수 없게 될 수도 있다.
  - 같은 페이지를 반복해서 가져오면 웹 서버 부담이 커지며, 웹 사이트를 압박하여 실제 사용자들이 사이트 접근을 할 수 없게 될 수도 있다.
  - 많은 수의 중복된 페이지들을 가져오면 결국 똑같은 페이지를 반환하는 인터넷 검색엔진이 된다.

### 1.5 빵 부스러기의 흔적

- 크롤링한 URL들은 굉장히 많기 때문에 어떤 URL을 방문했는지 판단하는 자료 구조를 사용할 필요가 있고, 속도와 메모리 사용 면에서 효과적이어야 한다.
- 로봇이 어떤 URL을 방문했는지 빠르게 결정하기 위해 검색 트리나 해시 테이블을 필요로 한다.
- 수억 개의 URL은 많은 공간을 차지한다.

**트리와 해시 테이블**

- 복잡한 로봇이라면 방문한 URL을 추적하기 위해 검색 트리나 해시 테이블을 사용했을 수도 있다.

**느슨한 존재 비트맵**

- 공간 사용의 최소화를 위해 존재 비트 배열(presence bit array)와 같은 느슨한 자료 구조를 사용한다.
  - 존재 비트는 각 URL을 해시 함수에 의해 고정된 크기의 숫자로 변환하여 배열 안에 대응하도록 하는 것

**체크포인트**

- 로봇 프로그램이 갑자기 중단될 경우를 대비하여 방문한 URL목록이 디스크에 저장되었는지 확인한다.

**파티셔닝**

- 한 대의 컴퓨터에서 하나의 로봇으로 크롤링을 완수하기에는 메모리, 디스크 공간, 연산 능력, 네트워크 대역폭이 충분하지 못할 수 있기 때문에, 각 분리된 여러 로봇들이 동시에 일하게 만든다.(farm)

### 1.6 별칭(alias)과 로봇 순환

- URL이 별칭을 가질 수 있기 때문에 올바른 자료 구조를 갖추었더라도 어떤 페이지를 이전에 방문했었는지 확인이 쉽지 않을 때도 있다.
  ![같은 문서를 가리키는 다른 URL들](https://user-images.githubusercontent.com/74203440/203156783-9f9c4086-fea8-4c14-9a9f-9edb340d39be.png)

### 1.7 URL 정규화하기

- 대부분의 웹 로봇들은 URL들을 표준 형식으로 '정규화' 함으로써 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거한다.

  - 1. 포트번호가 명시되지 않았다면, 호스트 명에 ':80'을 추가한다.
  - 2. 모든 %xx 이스케이핑된 문자들을 대응하는 문자로 변환한다.
  - 3. #태그들을 제거한다.

- URL 정규화로 기본적인 문법의 별칭을 제거할 수 있지만, 제거할 수 없는 다른 URL 별칭도 있다.

### 1.8 파일 시스템 링크 순환

- 심벌릭 링크는 디렉터리 계층을 만들어 끝없는 순환을 유발할 수 있다.
  ![심벌릭 링크 사이클](https://user-images.githubusercontent.com/74203440/203156818-e22cfa4f-e553-42c5-84da-18cc8296a963.png)

### 1.9 동적 가상 웹 공간

- 게이트웨이 애플리케이션은 같은 서버에 있는 가상의 URL에 대한 링크를 포함한 HTML을 즉석에서 만들어 낼 수 있다. 이러한 링크는 실제로 파일을 하나도 갖고 있지 않을 수도 있다. URL과 HTML이 매번 달라 보일 수 있기 때문에 로봇이 순환을 감지하기가 매우 어렵다.
  ![악의적인 동적 웹 공간의 예](https://user-images.githubusercontent.com/74203440/203156804-5eb174f0-fb32-459e-90dd-3a633755311a.jpeg)

### 1.10 루프와 중복 피하기

- 모든 순환을 피하는 완벽한 방법은 없고 웹에서 로봇이 문제를 일으킬 가능성은 많다.

**URL 정규화**

- URL을 표준 형태로 변환하여 같은 리소스를 가리키는 중복 URL을 거른다.

**너비 우선 크롤링**

- 방문할 URL들을 너비 우선으로 스케쥴링하면 순환의 영향을 최소화 할 수 있다.

**스로틀링**

- 웹 사이트의 별칭들에 지속적인 접근을 시도한다면, 스로틀링을 이용해 일정 시간동안의 해당 서버에 대한 접근 횟수와 중복의 총 횟수를 제한할 수 있다.

**URL 크기 제한**

- 보통 1KB를 넘는 긴 URL의 크롤링을 거부하는 방법이 있다. 길이 제한으로 순환이 중단될 수 있다.
- 이 기법은 가져오지 못하는 콘텐츠가 생길 수 있지만, URL이 특정 크기에 도달할 때마다 에러 로그를 남김으로써 특정 사이트에서 어떤 일이 벌어지는지 감시하는 사용자에게는 훌륭한 신호를 제공할 수 있다.

**URL/사이트 블랙리스트**

- 사람이 직접 개입하여 로봇 순환을 만들거나 함정인 사이트의 URL 목록을 만들어 관리하고 피한다.
- 대규모 크롤러들은 악의적이거나 문제를 내재한 것이 확실한 사이트들을 피하기 위해 사용하는 몇 가지 형태의 블랙리스트를 가지고 있다.

**패턴 발견**

- 심벌릭 링크를 통한 순환과 비슷한 오설정들은 일정 패턴을 따르는 경향이 있기 때문에 반복되는 구성요소를 가진 URL을 잠재적인 순환으로 보고 크롤링 하는 것을 거절한다.

**콘텐츠 지문(fingerprint)**

- 콘텐츠 지문을 사용하는 로봇들은 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬(checksum)을 계산한다.
- 이전에 보았던 체크섬을 가진 페이지를 가져오면 그 페이지는 크롤링하지 않는다.
- 지문 생성용으로 MD5와 같은 메시지 요약 함수가 있다.
- 웹 서버가 동적으로 페이지를 수정하는 경우 체크섬 계산에서 빠뜨리게 되어 중복 감지를 방해할 수 있다.

## 2. 로봇의 HTTP

- 로봇들은 다른 HTTP 클라이언트 프로그램과 다르지 않기 때문에 HTTP 명세를 지켜야 함
  - HTTP 요청을 만들며, 스스로를 HTTP/1.1 클라이언트라고 내세우기 때문에 적절한 HTTP 요청 헤더를 사용해야 함
- 많은 로봇들이 HTTP를 최소한으로만 구현하고자 함
  - 이 때문에 HTTP/1.0 요청을 보내는 로봇들이 많음

### 2.1 요청 헤더 식별하기

- HTTP를 최소한으로 지원하더라도, 신원 식별 헤더(특히 User-Agent HTTP 헤더)를 구현하고 전송해야 함
  - 로봇의 능력, 신원, 출신을 알려주는 기본적인 몇 가지 헤더들을 웹 사이트에게 보내줘야 함
- 이는 서버 측에서 잘못된 크롤러 사용자 검증과, 로봇이 다룰 수 있는 컨텐츠에 대한 정보를 얻기에 유용하게 해줌

**User-Agent**

- 서버에게 요청을 만든 로봇의 이름을 말해줌

**From**

- 로봇 사용자/관리자의 이메일 주소 제공(RFC 822 이메일 주소 포맷)

**Accept**

- 서버가 어떤 미디어 타입을 줄 수 있는지 말해줌

**Referer**

- 현재 요청 URL을 포함한 문서 URL 제공
- 서버로 하여금 어디에서 웹 사이트 링크를 찾아낼 수 있었는지에 대한 단서를 제공해줌

### 2.2 가상 호스팅

- 로봇 구현자들은 Host 헤더를 지원할 필요가 있음
- 가상 호스팅이 널리 퍼져있기 때문에, Host 헤더가 없으면 로봇은 URL에 대한 잘못된 컨텐츠를 찾게 됨
  - 이 때문에 HTTP/1.1은 Host 헤더 사용을 요구함

<img width="748" alt="Host 헤더의 부재" src="https://user-images.githubusercontent.com/75058239/202880621-73f0612f-340e-4a60-94cb-f78e81d727e2.png">

### 2.3 조건부 요청

- 수십억 개의 웹 페이지를 받아야 하는 인터넷 검색엔진 로봇의 경우, 오직 변경되었을 때만 컨텐츠를 가져오도록 할 수 있음
- 로봇은 받아둔 마지막 버전 이후 업데이트 사항이 있는지 알아보는 조건부 HTTP 요청을 구현함
  - HTTP 캐시가 리소스의 로컬 사본에 대한 유효성을 검사하는 방법과 매우 비슷

### 2.4 응답 다루기

- 사실 대다수 로봇들은 GET 메소드 위주로 사용하기 때문에 딱히 응답을 다룰 일이 없긴 함
- 그러나 조건부 요청을 사용하거나, 서버와의 상호작용을 더 잘 해보려고 하는 로봇들은 HTTP 응답을 다룰 줄 알아야 함

**_상태 코드_**

- 로봇들은 최소한 일반적인 상태 코드들은 다룰 수 있어야 함
  - 특히 200 OK, 404 Not Found
- 로봇이 명시적으로 이해할 수 없는 상태 코드는, 해당 상태 코드가 속한 분류에 근거해서 다뤄야 함
- 모든 서버가 항상 적절한 에러 코드를 반환하지 않는다는 점에 유의해야 함
  - 에러를 기술하는 메시지를 200 OK로 응답하는 서버도 있음
  - 이에 대비해서 뭔가 할 수 있는 일은 거의 없지만 명세를 구현하는 개발자라면 알아두고 있어야 함

**_엔티티_**

- HTTP 헤더에 임베딩된 정보를 따라서 로봇들은 엔티티 자체의 정보를 찾을 수 있음
- 메타 http-equiv 태그와 같은 메타 HTML 태그는 리소스에 대한 컨텐츠 저자가 포함시킨 정보
- http-equiv 태그 자체는 컨텐츠를 다루는 서버가 제공할 수도 있는 헤더를 덮어쓰기 위한 수단

```html
<meta http-equiv="Refresh" content="1; URL=index.html" />
```

※ Refresh 헤더를 HTML 내에서 제공해준다.

- http-equiv 태그를 헤더로 포함시키는 서버가 있는 반면, 그렇지 않은 서버도 있음
- 로봇 구현자는 HTML 문서의 HEAD 태그에서 http-equiv 정보를 찾고자 할 수 있음

### 2.5 User-Agent 타게팅

- 웹 관리자들은 로봇들로부터의 요청을 예상해야 함
- 서버에서 브라우저의 종류에 맞게 컨텐츠를 최적화하는 경우,<br>로봇에게 "your browser does not support frames"라는 에러 문구를 전송하게 될 수 있음
- 그러므로 사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야 함
  - 몇몇 브라우저에 특화된 컨텐츠를 개발하는 대신, 보다 다양한 클라이언트에게 대응할 수 있는 유연한 페이지 개발
  - 최소한 로봇이 방문했는데 컨텐츠를 못 얻게 되는 일이 없도록 대비해야 함

<br>

## 3. 부적절하게 동작하는 로봇들

**폭주하는 로봇**

- 로봇은 웹 서핑을 하는 사람보다 훨씬 빠르게 HTTP 요청을 만들 수 있음
  - 게다가 빠른 네트워크 연결을 갖춘 빠른 컴퓨터에서 동작하고 있을 가능성이 높음
- 빠른 속도를 가졌기 때문에 논리적 에러나 순환에 빠졌을 때는 웹 서버에 극심한 과부하를 안겨주게 될 수도 있음
- 그렇기 때문에 로봇 구현자들은 폭주 방지를 위한 보호 장치를 마련해야만 함

**오래된 URL**

- 로봇은 URL의 목록을 방문하는데, 해당 목록이 오래되었을 수 있음
- 웹 사이트의 컨텐츠가 많이 바뀌었다면, 로봇은 존재하지 않는 URL에 대한 요청을 많이 보내게 될 수도 있음
- 이 또한 에러 페이지 제공으로 인한 서버 부하로 이어질 수 있기 때문에 웹 사이트 관리자의 짜증을 유발함

**길고 잘못된 URL**

- 순환이나 로직상의 오류로 로봇은 웹 사이트에게 크고 의미 없는 URL을 요청하게 될 수도 있음
- 이는 웹 서버 처리 능력에 영향을 주고, 웹 서버의 접근 로그를 어지럽게 채워버리고, 허술한 웹 서버라면 고장을 낼 수도 있음

**호기심이 지나친 로봇**

- 어떤 로봇들은 사적인 데이터에 대한 URL을 얻어서 인터넷 검색엔진에서도 접근할 수 있도록 만들 수도 있음
  - 최악의 경우 사생활 침해라고 여겨질 수 있음
- 보통은 사적 컨텐츠에 대한 하이퍼링크를 잘 감춰두지 않아서 접근이 되는 것이긴 한데,<br>매우 광적인 로봇은 하이퍼링크가 명시적으로 존재하지도 않는 문서들을 디렉토리의 컨텐츠를 가져오는 등의 방법으로 찾아내기도 함
- 로봇 구현자는 비밀번호 파일, 신용카드 정보 등 민감한 데이터를 로봇이 검색하지 않도록 주의해야 함
- 사실, 인터넷에 링크가 존재하는 한 진정한 의미의 사적인 리소스는 거의 없음

**동적 게이트웨이 접근**

- 로봇들은 그들이 접근하고 있는 것에 대해 언제나 잘 알고 있는 것은 아님
- 로봇은 게이트웨이 애플리케이션의 컨텐츠에 대한 URL로 요청하게 될 수도 있음
  - 이 경우 데이터는 아마 특수 목적을 위한 것일 테고 처리 비용이 많이 듦
- 많은 웹 사이트 관리자들은 게이트웨이에서 얻은 문서를 요청하는 순진한 로봇들을 좋아하지 않음

### 9.4 로봇 차단하기
![Logo](http://www.robotstxt.org/static/robotstxt.gif)

- Robots Exclusion Standard 라는 이름의 표준(실제로 표준은 아님) 로봇의 동작을 더 잘 제어할 수 있는 메커니즘을 제공하는 기법을 제안
- 로봇의 접근을 제어하는 정보를 저장하는 파일의 이름을 따서 `robots.txt`라고 부른다.

> 1. 서버 문서 루트에 `robots.txt`라는 이름의 파일을 제공. `robots.txt`은 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보가 담겨있음
> 2. 로봇은 웹 사이트의 다른 리소스에 접근하기 전 `robots.txt`를 먼저 요청
> 3. `robots.txt`에 명시된 권한정보에 따라 접근이 허용/불허 처리 된다.

#### 9.4.1 로봇 차단 표준
- 로봇 차단 표은 임시방편으로 마련된 표준. (표준은 아니지만 굉장히 광범위하게 사용중)
- 대부분의 주류 업체들과 검색엔진 크롤러들은 이 차단 표준을 지원

#### 9.4.2 웹 사이트와 robots.txt 파일들
- 웹 사이트의 어떤 URL을 방문하기 전, 그 웹사이트에 `robots.txt` 파일이 존재하면 로봇은 반드시 그 파일을 가져와서 처리해야 함.
- `robots.txt` 파일은 단 하나만 존재하며 만약 가상 호스팅 되는 경우 각각의 가상 docroot에 서로 다른 `robots.txt`가 있을 수 있다.

**`robots.txt` 가져오기**
- HTTP GET 메서드를 통해 `robots.txt` 리소스를 가져온다.
- `robots.txt`가 존재하면 서버는 text/plain 본문으로 반환
- 서버가 404 상태코드로 응답하면 로봇은 그 서버는 로봇의 접근을 제한하지 않는 것으로 간주(`robots.txt`이 없으면 싹 다 긁어갈 수 있음)

- 로봇은 From, User-Agent 등의 헤더를 통해 신원 ㅈ어보를 넘기고 사이트 관리자가 로봇에 대해 문의나 불만사항이 있을 경우를 위해 연락처를 제공해야 한다.
  ```http
  GET /robots.txt HTTP/1.0
  Host: www.google.com
  User-Agent: Slurp/2.0 
  Date: Sun Dec 4 20:22:48 KST 2022
  ```

**응답코드**
- 로봇은 어떤 웹 사이트든 반드시 `robots.txt`를 먼저 찾아보고 상태코드에 따라 다르게 동작한다.
- 2xx: `robots.txt`를 파싱해 차단 규칙을 얻고 규칙에 따라서 사이트에서 데이터를 가져온다.
- 3xx: 리소스가 발견될 때까지 리다이렉트를 따라간다.
- 404: 로봇은 차단규칙이 없다고 가정하고 `robots.txt`의 제약 없이 사이트의 모든 데이터를 가져올 수 있다.
- 401 or 403: 해당 사이트로의 접근이 완전히 제한됨
- 503: 사이트의 리소스를 검색하는 것을 뒤로 미룬다.

#### 9.4.3 `robots.txt` 파일 포맷
[Google robots.txt](https://www.google.com/robots.txt)

**User-Agent**
- 각 로봇의 레코드는 **하나 이상**의 User-Agent 줄로 시작
```
User-Agent: <robot-name>
```
혹은
```
User-Agent: *
```
으로 시작한다. 

- 만약 로봇 이름에 대응하는 User-Agent 줄을 찾지 못하고, 와일드카드(*) 줄도 찾지 못하면 대응하는 레코드가 없는 것이므로 접근에 제한이 없다.
- 로봇 이름을 대소부자를 구분하지 않는 부분 문자여로가 대응하므로 주의해야 한다.
  (ex. User-Agent: 'bot'sms Bot, Robot, Bottom-Feeder, Spambot 등에 매칭됨)

**Disallow/Allow**
- Disallow와 Allow 줄은 User-Agent 바로 뒤에 온다.
- 특정 로봇에 대해 어떤 URL 경로가 명시적으로 금지되어 있고 허용되는지 기술한다.

**Disallow/Allow 접두 매칭(prefix matching)**
- Disallow, Allow 규칙이 어떤 경로에 적용되려면, 그 경로의 시작부터 규칙 경로의 길이만큼의 문자열이 규칙 경로와 같아야 함. 그리고 대소문자 차이도 없어야 함. * 의 경우, 특별한 의미는 없지만, 대신 빈 문자열을 이용해 모든 문자열에 매치되도록 할 수 있음.
- 규칙 경로나 URL 경로의 임의의 이스케이핑된 문자들(%XX)은 비교 전에 원래대로 복원됨. 단, `/` 를 의미하는 `%2F`는 예외이며, 반드시 그대로 매치되어야 함.
- 만약 어떤 규칙 경로가 빈 문자열이라면, 그 규칙은 모든 URL 경로와 매치됨.

#### 9.4.4 그 외에 알아둘 점
- 명세가 발전함에 따라 User-Agent, Disallow, Allow 외의 다른 필드를 포함할 수 있음. 로봇은 자신이 이해하지 못하는 필드는 무시함.
- 하위 호환성을 위해, 한 줄을 여러 줄로 나누어 적는 것은 허용되지 않는다.
- User-Agent : *\nDisallow:/ 이런 식이 되면 곤란하다.
- 주석은 파일의 어디에서든 허용됨.
- robots.txt 0.0 버전은 Allow 줄을 지원하지 않았음.

#### 9.4.5 robots.txt의 캐싱과 만료
- 매 파일 접근마다 로봇이 `robots.txt` 파일을 새로 가져올 순 없으므로 당연히 캐시가 가능하다. 
- 로봇은 `robots.txt`파일 사본이 만료될 때까지 로봇에 의해 사용되고 캐싱을 제어하기 위해 표준 HTTP 캐시 제어 메커니즘이 원 서버와 로봇 양쪽 모두에 의해 사용된다.

- 로봇 명세 초안은 Cache-Control 지시자가 존재하는 경우 7일간 캐싱하도록 하고 있다. (실무 입장에서 너무 길다.)

#### 9.4.6 로봇 차단 펄 코드
`robots.txt` 파일과 상호작용 하는 공개된 펄(Perl) 라이브러리가 존재한다. (ex. WWW::RobustRules 모듈)


#### 9.4.7 HTML 로봇 제어 META 테그
- `robots.txt` 파일의 단점 중 하나는 그 파일을 콘텐츠의 작성자 개개인이 아니라 웹 사이트 관리자가 소유한다는 것이다.
- HTML 페이지 저자는 로봇 제어 태그 `META`를 이용해 로봇이 개별 페이지에 접근하는 것을 제한하는 좀 더 직접적인 방법을 제안한다. 


**로봇 META 지시자**

- NOINDEX
이 페이지를 처리하지 말고 무시(이 콘텐츠를 색인이나 데이터베이스에 포함시키지 말 것)
  ```
  <META NAME="ROBOTS" CONTENT="NOINDEX">
  ```
- NOFOLLOW
로봇에게 이 페이지가 링크한 페이지를 크롤링하지 말라고 말하는 것.
  ```
  <META NAME="ROBOTS" CONTENT="NOFOLLOW">
  ```
- INDEX
이 페이지의 콘텐츠를 인덱싱해도 된다고 말해주기.
  ```
  <META NAME="ROBOTS" CONTENT="INDEX">
  ```
- FOLLOW
이 페이지가 링크한 페이지를 크롤링해도 된다고 말해주기.
  ```
  <META NAME="ROBOTS" CONTENT="FOLLOW">
  ```
- NOARCHIVE
이 페이지의 캐시를 위한 로컬 사본을 만들어서는 안 된다고 말해주기.
- ALL
INDEX, FOLLOW와 같음.
- NONE
NOINDEX, NOFOLLOW와 같음.

**검색엔진 META 태그**
- 모든 로봇 MEAT 태그는 name = "robots" 속성을 포함함.
위에 소개된 것 외에도 DESCRIPTION, KEYWORDS 같은 콘텐츠의 색인을 만드는 검색엔진 로봇들에 대해 유용함.

<img width="781" alt="추가 META 태그 지시자" src="https://user-images.githubusercontent.com/87509645/205501545-1d23ca1d-e65e-4b53-b451-ff8560f9b3f3.png">
